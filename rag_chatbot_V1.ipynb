{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonardoLuca/AskYou/blob/main/rag_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh8uXjXxbQFF"
      },
      "source": [
        "# Teste conexão Gemini\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KZFMCJ_yKkTZ"
      },
      "outputs": [],
      "source": [
        "# Célula 1: Instalar as bibliotecas necessárias\n",
        "!pip install -q langchain langchain-google-genai datasets qdrant-client tiktoken\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-PfPlKnggFJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06Lg-QRmL67q"
      },
      "outputs": [],
      "source": [
        "# Célula 2: Importar bibliotecas e configurar a API Key do Google\n",
        "import os\n",
        "import sys\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Variável para verificar se a configuração foi bem sucedida\n",
        "google_api_key_configurada = False\n",
        "chat = None # Inicializa a variável chat\n",
        "\n",
        "try:\n",
        "    # Pega a chave de API dos Secrets do Colab\n",
        "    # Certifique-se que o nome do secret é 'GOOGLE_API_KEY'\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "    # Verifica se a chave foi realmente obtida (não é None ou vazia)\n",
        "    if not GOOGLE_API_KEY:\n",
        "        print(\"Erro: A chave de API 'GOOGLE_API_KEY' foi encontrada nos Secrets, mas está vazia.\", file=sys.stderr)\n",
        "    else:\n",
        "        print(\"Chave de API do Google carregada com sucesso!\")\n",
        "        google_api_key_configurada = True\n",
        "\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"Erro: Secret 'GOOGLE_API_KEY' não encontrado.\", file=sys.stderr)\n",
        "    print(\"Por favor, adicione sua chave de API do Google AI Studio aos Secrets do Colab.\", file=sys.stderr)\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro inesperado ao buscar a chave de API: {e}\", file=sys.stderr)\n",
        "\n",
        "# Limpa a variável do ambiente se ela existir por algum motivo (boa prática)\n",
        "# Embora estejamos passando diretamente, é bom garantir que não haja conflito.\n",
        "if 'GOOGLE_API_KEY' in os.environ:\n",
        "    del os.environ['GOOGLE_API_KEY']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_sy9Z4FNLwn"
      },
      "outputs": [],
      "source": [
        "# Célula 3: Instanciar o modelo Gemini (só executa se a chave foi carregada)\n",
        "\n",
        "if google_api_key_configurada:\n",
        "    try:\n",
        "        # Escolha o modelo Gemini (ex: 'gemini-1.5-pro-latest', 'gemini-1.5-flash-latest')\n",
        "        chat = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-pro-latest\",  # Ou outro modelo Gemini compatível\n",
        "            google_api_key=GOOGLE_API_KEY,\n",
        "            # convert_system_message_to_human=True # Descomente se tiver problemas com SystemMessage\n",
        "                                                 # Isso faz com que a mensagem do sistema seja anexada\n",
        "                                                 # à primeira mensagem humana, o que às vezes funciona melhor com Gemini.\n",
        "        )\n",
        "        print(f\"Modelo {chat.model} instanciado com sucesso.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao instanciar o modelo ChatGoogleGenerativeAI: {e}\", file=sys.stderr)\n",
        "        chat = None # Garante que chat é None se a instanciação falhar\n",
        "else:\n",
        "    print(\"\\nInstanciação do modelo pulada pois a chave de API não foi configurada.\", file=sys.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQ6-pvMpMS-H"
      },
      "outputs": [],
      "source": [
        "# Célula 4: Definir as mensagens (geralmente sem alterações)\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Você é um assistente útil que responde perguntas.\"),\n",
        "    HumanMessage(content=\"Olá Bot, como você está hoje?\"),\n",
        "    AIMessage(content=\"Estou bem, obrigado por perguntar! Como posso ajudar?\"),\n",
        "    HumanMessage(content=\"Gostaria de entender o que é Machine Learning\")\n",
        "]\n",
        "\n",
        "print(\"Lista de mensagens definida.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sT-kwz8Nnwv"
      },
      "outputs": [],
      "source": [
        "# Célula 5: Invocar o modelo e obter a resposta (com verificação)\n",
        "\n",
        "res = None # Inicializa a variável de resultado\n",
        "\n",
        "if chat: # Verifica se o modelo foi instanciado com sucesso\n",
        "    try:\n",
        "        print(f\"\\nEnviando mensagens para o modelo {chat.model}...\")\n",
        "        res = chat.invoke(messages)\n",
        "        print(\"Resposta recebida do modelo.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Erro durante a invocação do modelo: {e}\", file=sys.stderr)\n",
        "else:\n",
        "    print(\"\\nInvocação pulada pois o modelo não foi instanciado corretamente (verifique a API Key e a Célula 3).\", file=sys.stderr)\n",
        "\n",
        "# Exibe o objeto de resposta completo (opcional, útil para debug)\n",
        "# print(\"\\nObjeto de resposta completo:\")\n",
        "# print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2rXuQw_fSww3"
      },
      "outputs": [],
      "source": [
        "# Célula 6: Imprimir o conteúdo da resposta (com verificação)\n",
        "\n",
        "if res: # Verifica se houve uma resposta válida\n",
        "    print(\"\\n--- Conteúdo da Resposta ---\")\n",
        "    print(res.content)\n",
        "    print(\"--------------------------\")\n",
        "elif chat: # Se o chat foi instanciado mas res é None, houve erro na invocação\n",
        "     print(\"\\nNão foi possível obter uma resposta do modelo (verifique o erro na Célula 5).\", file=sys.stderr)\n",
        "else:\n",
        "     print(\"\\nNão há resposta para exibir pois o modelo não foi instanciado.\", file=sys.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJBa5ozzaMPy"
      },
      "outputs": [],
      "source": [
        "messages.append(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lgQckagFaas8"
      },
      "outputs": [],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqVSNLfVaj8s"
      },
      "outputs": [],
      "source": [
        "prompt = HumanMessage (\n",
        "    content=\"Qual a diferença entre supervisionado e não supervisionado?\"\n",
        "    )\n",
        "messages.append(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PaY-r35pbBNS"
      },
      "outputs": [],
      "source": [
        "res = chat.invoke(messages)\n",
        "print(res.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83g6EmaEbIBA"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbBqHD05beJD"
      },
      "outputs": [],
      "source": [
        "#Conectando com o google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gdVk5BhciWZ"
      },
      "outputs": [],
      "source": [
        "#Importando o dataset do drive\n",
        "import json\n",
        "\n",
        "# Path to your JSON file in Google Drive\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/datasets/dataset-cv-leonardo.json'\n",
        "\n",
        "# Load the JSON file\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Display the first entry to verify\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ON5EsX1RgjqN"
      },
      "outputs": [],
      "source": [
        "#Bibliotecas para embedding\n",
        "!pip install -q sentence-transformers faiss-cpu transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mnTiVrlPhRMk"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Load the embedding model\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Extract descriptions from the dataset\n",
        "descriptions = [entry['Description'] for entry in dataset]\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = embedder.encode(descriptions, show_progress_bar=True)\n",
        "\n",
        "# Convert to numpy array for FAISS\n",
        "embeddings = np.array(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hvCpjZmIi9rn"
      },
      "outputs": [],
      "source": [
        "print(descriptions[0])\n",
        "print(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EET0zQ12jgZ0"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "# Create a FAISS index\n",
        "dimension = embeddings.shape[1]  # Embedding dimension\n",
        "index = faiss.IndexFlatL2(dimension)  # L2 distance for similarity\n",
        "\n",
        "# Add embeddings to the index\n",
        "index.add(embeddings)\n",
        "\n",
        "# Save the index (optional)\n",
        "faiss.write_index(index, '/content/drive/My Drive/Colab Notebooks/index/faiss-cv-leonardo-index.bin')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu2mmqRplIEs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "b-Xw50xOmCKO"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'transformers'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Load a Generative Model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load a generative model. Utilizando modelo do chat gpt, que não estava comaptivel com o primeiro exemplo.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#f\u001b[39;00m\n\u001b[32m      6\u001b[39m generator = pipeline(\u001b[33m'\u001b[39m\u001b[33mtext-generation\u001b[39m\u001b[33m'\u001b[39m, model=\u001b[33m'\u001b[39m\u001b[33mdistilgpt2\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers'"
          ]
        }
      ],
      "source": [
        "#Load a Generative Model\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load a generative model. Utilizando modelo do chat gpt, que não estava comaptivel com o primeiro exemplo.\n",
        "#Foi necessário alterar o código para integrar com o exemplo do GEMINI\n",
        "generator = pipeline('text-generation', model='distilgpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D7s1FiApCDI"
      },
      "outputs": [],
      "source": [
        "#Implement the RAG Pipeline\n",
        "\n",
        "def rag_query(query, k=3):\n",
        "    # Encode the query\n",
        "    query_embedding = embedder.encode([query])\n",
        "\n",
        "    # Search for the top-k most similar entries\n",
        "    distances, indices = index.search(np.array(query_embedding), k)\n",
        "\n",
        "    # Retrieve corresponding dataset entries\n",
        "    retrieved_entries = [dataset[idx] for idx in indices[0]]\n",
        "\n",
        "    # Create a context from retrieved entries\n",
        "    context = \"\\n\".join([entry['Description'] for entry in retrieved_entries])\n",
        "\n",
        "    # Generate a response\n",
        "    prompt = f\"Baseado nessa informação a seguir:\\n{context}\\n\\nResponda a questão: {query}\"\n",
        "    response = generator(prompt, max_length=200, num_return_sequences=1)[0]['generated_text']\n",
        "\n",
        "    return response, retrieved_entries\n",
        "\n",
        "# Example query\n",
        "query = \"Qual a experiência de Leonardo com Python?\"\n",
        "response, retrieved = rag_query(query)\n",
        "print(\"Respostas:\", response)\n",
        "print(\"Retrieved Entries:\", retrieved)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNWY7p/rzRrSZmTAL3ENfW6",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNd4bvPzJvI6iFehpPDLeVc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "957300fa06ea44188fc1e7b5ee55de7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d73872c1a5804c50a634931a9dcb89b3",
              "IPY_MODEL_ec141abd9c8d4215ac83e5731299a879",
              "IPY_MODEL_732c436cca4c40c4a33110e8a08770dd"
            ],
            "layout": "IPY_MODEL_8a150b9db0ac4b8d8258af314fbf40cd"
          }
        },
        "d73872c1a5804c50a634931a9dcb89b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90a0af912fba435bb6e6b17654786592",
            "placeholder": "​",
            "style": "IPY_MODEL_4961d32c9f4b48778aba8a6856f58455",
            "value": "Batches: 100%"
          }
        },
        "ec141abd9c8d4215ac83e5731299a879": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c58c62c5bc947efb4346a7ae55482e8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_150e1b27f8494cc58420884fcd62ee81",
            "value": 1
          }
        },
        "732c436cca4c40c4a33110e8a08770dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f686384b0b754f33a8bf21718f5b46f7",
            "placeholder": "​",
            "style": "IPY_MODEL_8b276b00d67f413ea03e3c15e5ae7e5f",
            "value": " 1/1 [00:05&lt;00:00,  5.86s/it]"
          }
        },
        "8a150b9db0ac4b8d8258af314fbf40cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90a0af912fba435bb6e6b17654786592": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4961d32c9f4b48778aba8a6856f58455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c58c62c5bc947efb4346a7ae55482e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "150e1b27f8494cc58420884fcd62ee81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f686384b0b754f33a8bf21718f5b46f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b276b00d67f413ea03e3c15e5ae7e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonardoLuca/AskYou/blob/main/rag_chatbot_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook Colab com RAG e Gemini Integrados"
      ],
      "metadata": {
        "id": "Hh8uXjXxbQFF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KZFMCJ_yKkTZ"
      },
      "outputs": [],
      "source": [
        "# CÉLULA 1: INSTALAÇÕES\n",
        "\n",
        "# Instala todas as bibliotecas necessárias de uma vez\n",
        "!pip install -q langchain langchain-google-genai sentence-transformers faiss-cpu torch google-colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B-PfPlKnggFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 2: IMPORTS E CONFIGURAÇÃO DA API KEY\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import faiss\n",
        "from google.colab import userdata, drive\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# --- Configuração da Chave de API do Google ---\n",
        "GOOGLE_API_KEY = None\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    if not GOOGLE_API_KEY:\n",
        "        print(\"AVISO: Chave 'GOOGLE_API_KEY' encontrada nos Secrets, mas está vazia.\", file=sys.stderr)\n",
        "        GOOGLE_API_KEY = None # Garante que é None se estiver vazia\n",
        "    else:\n",
        "        print(\"Chave de API do Google carregada com sucesso!\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"ERRO: Secret 'GOOGLE_API_KEY' não encontrado.\", file=sys.stderr)\n",
        "    print(\"Por favor, adicione sua chave de API do Google AI Studio aos Secrets do Colab.\", file=sys.stderr)\n",
        "except Exception as e:\n",
        "    print(f\"ERRO ao buscar a chave de API: {e}\", file=sys.stderr)\n",
        "\n",
        "# Limpa a variável de ambiente se existir (boa prática)\n",
        "if 'GOOGLE_API_KEY' in os.environ:\n",
        "    del os.environ['GOOGLE_API_KEY']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06Lg-QRmL67q",
        "outputId": "7cf6ef71-82af-42c6-a5bc-bbe197026a12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chave de API do Google carregada com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 3: INICIALIZAÇÃO DOS MODELOS (GEMINI E EMBEDDING)\n",
        "\n",
        "# --- Inicializa o Modelo Gemini ---\n",
        "chat_model = None\n",
        "if GOOGLE_API_KEY:\n",
        "    try:\n",
        "        # Use o modelo Gemini desejado (Pro ou Flash)\n",
        "        chat_model = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-1.5-pro-latest\", # Ou \"gemini-1.5-flash-latest\" para mais rápido/barato\n",
        "            google_api_key=GOOGLE_API_KEY,\n",
        "            temperature=0.2, # Reduz a criatividade para respostas mais factuais Variabilidade\n",
        "            # safety_settings=... # Adicione configurações de segurança se necessário\n",
        "        )\n",
        "        print(f\"Modelo Gemini '{chat_model.model}' instanciado com sucesso.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERRO ao instanciar o modelo ChatGoogleGenerativeAI: {e}\", file=sys.stderr)\n",
        "else:\n",
        "    print(\"AVISO: Modelo Gemini não será instanciado pois a chave de API não foi carregada.\", file=sys.stderr)\n",
        "\n",
        "# --- Inicializa o Modelo de Embedding ---\n",
        "embedder_model = None\n",
        "try:\n",
        "    print(\"Carregando modelo de embedding (SentenceTransformer)...\")\n",
        "    start_time = time.time()\n",
        "    # Modelo leve e eficaz para semântica geral\n",
        "    embedder_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    print(f\"Modelo de embedding carregado em {time.time() - start_time:.2f} segundos.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERRO ao carregar o modelo de embedding: {e}\", file=sys.stderr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_sy9Z4FNLwn",
        "outputId": "2c22cf1b-ef26-480d-e86e-462528f28f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo Gemini 'models/gemini-1.5-pro-latest' instanciado com sucesso.\n",
            "Carregando modelo de embedding (SentenceTransformer)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo de embedding carregado em 4.00 segundos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================\n",
        "#             CÉLULA 4: MONTAGEM DO DRIVE E CARREGAMENTO/INDEXAÇÃO DO CV (COM IndexFlatIP)\n",
        "# ===================================================\n",
        "\n",
        "# --- Monta o Google Drive ---\n",
        "# (Como estava antes)\n",
        "try:\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "    print(\"Google Drive montado com sucesso em /content/drive.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERRO ao montar o Google Drive: {e}\", file=sys.stderr)\n",
        "\n",
        "# --- Define Caminhos (AJUSTE SE NECESSÁRIO) ---\n",
        "# (Como estava antes)\n",
        "DRIVE_BASE_PATH = \"/content/drive/My Drive/Colab Notebooks\" # Exemplo, ajuste!\n",
        "CV_JSON_PATH = os.path.join(DRIVE_BASE_PATH, \"datasets/dataset-cv-leonardo.json\")\n",
        "# **Mudar o nome do arquivo de índice para refletir a mudança (opcional, mas recomendado)**\n",
        "FAISS_INDEX_PATH = os.path.join(DRIVE_BASE_PATH, \"index/faiss-cv-leonardo-index-IP.bin\") # <-- MUDOU O NOME\n",
        "\n",
        "# --- Variáveis para os dados e índice ---\n",
        "# (Como estava antes)\n",
        "cv_data = None\n",
        "cv_descriptions = None\n",
        "faiss_index = None\n",
        "\n",
        "# --- Carrega o Dataset (CV JSON) ---\n",
        "# (Como estava antes - carregar cv_data e cv_descriptions)\n",
        "if os.path.exists(CV_JSON_PATH):\n",
        "    try:\n",
        "        with open(CV_JSON_PATH, 'r', encoding='utf-8') as f:\n",
        "            cv_data = json.load(f)\n",
        "        print(f\"Dataset CV carregado de: {CV_JSON_PATH}\")\n",
        "        cv_descriptions = [entry.get('Description', '') for entry in cv_data]\n",
        "        print(f\"Extraídas {len(cv_descriptions)} descrições do CV.\")\n",
        "        if not cv_descriptions:\n",
        "             print(\"AVISO: Nenhuma descrição encontrada no arquivo JSON.\", file=sys.stderr)\n",
        "             cv_data = None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"ERRO: O arquivo {CV_JSON_PATH} não é um JSON válido.\", file=sys.stderr)\n",
        "    except Exception as e:\n",
        "        print(f\"ERRO ao carregar ou processar o arquivo JSON do CV: {e}\", file=sys.stderr)\n",
        "else:\n",
        "    print(f\"ERRO: Arquivo do dataset CV não encontrado em: {CV_JSON_PATH}. Verifique o caminho.\", file=sys.stderr)\n",
        "\n",
        "\n",
        "# --- Cria/Carrega Embeddings e Índice FAISS (usando IndexFlatIP) ---\n",
        "if cv_data and embedder_model:\n",
        "    try:\n",
        "        # Verifica se o índice FAISS (versão IP) já existe\n",
        "        if os.path.exists(FAISS_INDEX_PATH):\n",
        "            print(f\"Tentando carregar índice FAISS existente (IP) de: {FAISS_INDEX_PATH}\")\n",
        "            faiss_index = faiss.read_index(FAISS_INDEX_PATH)\n",
        "            # Validações (como antes)\n",
        "            if faiss_index.d != embedder_model.get_sentence_embedding_dimension():\n",
        "                 print(f\"AVISO: Dimensão do índice ({faiss_index.d}) diferente. Recriando.\", file=sys.stderr)\n",
        "                 faiss_index = None\n",
        "            elif faiss_index.ntotal != len(cv_descriptions):\n",
        "                 print(f\"AVISO: Número de vetores ({faiss_index.ntotal}) diferente. Recriando.\", file=sys.stderr)\n",
        "                 faiss_index = None\n",
        "            else:\n",
        "                 print(f\"Índice FAISS (IP) carregado com sucesso ({faiss_index.ntotal} vetores).\")\n",
        "\n",
        "        # Se não carregou, cria um novo\n",
        "        if faiss_index is None:\n",
        "            print(\"Gerando embeddings para as descrições do CV...\")\n",
        "            start_time = time.time()\n",
        "            embeddings = embedder_model.encode(cv_descriptions, show_progress_bar=True)\n",
        "            embeddings = np.array(embeddings).astype('float32')\n",
        "\n",
        "            # **** NORMALIZAÇÃO DOS EMBEDDINGS ****\n",
        "            print(\"Normalizando embeddings (L2 norm)...\")\n",
        "            faiss.normalize_L2(embeddings)\n",
        "            # *************************************\n",
        "\n",
        "            dimension = embeddings.shape[1]\n",
        "            print(f\"Criando novo índice FAISS com IndexFlatIP (Similaridade Cosseno), dimensão {dimension}...\")\n",
        "            # **** USAR IndexFlatIP ****\n",
        "            faiss_index = faiss.IndexFlatIP(dimension)\n",
        "            # **************************\n",
        "            faiss_index.add(embeddings) # Adiciona os embeddings NORMALIZADOS\n",
        "            print(f\"Índice FAISS (IP) criado e populado com {faiss_index.ntotal} vetores.\")\n",
        "\n",
        "            # Salva o novo índice (IP)\n",
        "            try:\n",
        "                print(f\"Salvando índice FAISS (IP) em: {FAISS_INDEX_PATH}\")\n",
        "                os.makedirs(os.path.dirname(FAISS_INDEX_PATH), exist_ok=True)\n",
        "                faiss.write_index(faiss_index, FAISS_INDEX_PATH)\n",
        "                print(\"Índice FAISS (IP) salvo com sucesso.\")\n",
        "            except Exception as e:\n",
        "                print(f\"ERRO ao salvar o índice FAISS (IP): {e}\", file=sys.stderr)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERRO durante a geração de embeddings ou manipulação do índice FAISS (IP): {e}\", file=sys.stderr)\n",
        "        faiss_index = None\n",
        "\n",
        "elif not embedder_model:\n",
        "     print(\"AVISO: Processo de indexação pulado (modelo de embedding não carregado).\", file=sys.stderr)\n",
        "else:\n",
        "     print(\"AVISO: Processo de indexação pulado (dados do CV não carregados).\", file=sys.stderr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "957300fa06ea44188fc1e7b5ee55de7f",
            "d73872c1a5804c50a634931a9dcb89b3",
            "ec141abd9c8d4215ac83e5731299a879",
            "732c436cca4c40c4a33110e8a08770dd",
            "8a150b9db0ac4b8d8258af314fbf40cd",
            "90a0af912fba435bb6e6b17654786592",
            "4961d32c9f4b48778aba8a6856f58455",
            "1c58c62c5bc947efb4346a7ae55482e8",
            "150e1b27f8494cc58420884fcd62ee81",
            "f686384b0b754f33a8bf21718f5b46f7",
            "8b276b00d67f413ea03e3c15e5ae7e5f"
          ]
        },
        "id": "tQ6-pvMpMS-H",
        "outputId": "970042c4-4134-470c-a0e0-c4b2f0b44dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive montado com sucesso em /content/drive.\n",
            "Dataset CV carregado de: /content/drive/My Drive/Colab Notebooks/datasets/dataset-cv-leonardo.json\n",
            "Extraídas 27 descrições do CV.\n",
            "Gerando embeddings para as descrições do CV...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "957300fa06ea44188fc1e7b5ee55de7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalizando embeddings (L2 norm)...\n",
            "Criando novo índice FAISS com IndexFlatIP (Similaridade Cosseno), dimensão 384...\n",
            "Índice FAISS (IP) criado e populado com 27 vetores.\n",
            "Salvando índice FAISS (IP) em: /content/drive/My Drive/Colab Notebooks/index/faiss-cv-leonardo-index-IP.bin\n",
            "Índice FAISS (IP) salvo com sucesso.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================\n",
        "#             CÉLULA 5: FUNÇÃO RAG PRINCIPAL (COM MAIS DEBUG)\n",
        "# ===================================================\n",
        "\n",
        "def ask_cv_assistant(query, k=3, similarity_threshold=0.45): # <-- Ajustado threshold para Cosine\n",
        "    \"\"\"\n",
        "    Realiza uma consulta RAG no CV carregado.\n",
        "    USA SIMILARIDADE COSSENO (requer IndexFlatIP e normalização - veja Célula 4 modificada)\n",
        "\n",
        "    Args:\n",
        "        query (str): A pergunta do usuário sobre o CV.\n",
        "        k (int): Número máximo de seções relevantes a recuperar.\n",
        "        similarity_threshold (float): Limiar de similaridade cosseno (0 a 1).\n",
        "                                      Valores mais altos indicam maior similaridade.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (str: resposta_gerada, list: documentos_recuperados)\n",
        "               Retorna mensagens de erro na string de resposta em caso de falha.\n",
        "    \"\"\"\n",
        "    # --- Verificações Iniciais ---\n",
        "    # (Mantenha as verificações como estavam)\n",
        "    if not chat_model: return \"ERRO: O modelo Gemini não está pronto.\", []\n",
        "    if not embedder_model: return \"ERRO: O modelo de embedding não está pronto.\", []\n",
        "    if not faiss_index: return \"ERRO: O índice FAISS (CV) não está pronto.\", []\n",
        "    if not cv_data: return \"ERRO: Os dados do CV não foram carregados.\", []\n",
        "\n",
        "    print(f\"\\n[RAG] Processando consulta: '{query}'\")\n",
        "\n",
        "    try:\n",
        "        # --- 1. Codificar a Consulta e NORMALIZAR ---\n",
        "        start_time = time.time()\n",
        "        query_embedding = embedder_model.encode([query])\n",
        "        query_embedding = np.array(query_embedding).astype('float32')\n",
        "        faiss.normalize_L2(query_embedding) # Normaliza para usar com IndexFlatIP (Cosine Similarity)\n",
        "        print(f\"[RAG] Consulta codificada e normalizada em {time.time() - start_time:.3f} seg.\")\n",
        "\n",
        "        # --- 2. Buscar no FAISS (usando IndexFlatIP) ---\n",
        "        start_time = time.time()\n",
        "        # search retorna similaridades (Inner Product/Cosine) e índices\n",
        "        similarities, indices = faiss_index.search(query_embedding, k)\n",
        "        print(f\"[RAG] Busca FAISS concluída em {time.time() - start_time:.3f} seg. Encontrados {len(indices[0])} vizinhos.\")\n",
        "\n",
        "        # --- 3. Recuperar e Filtrar Documentos (usando similaridade) ---\n",
        "        retrieved_docs_info = []\n",
        "        relevant_indices_found = [] # Para evitar duplicatas se k for grande\n",
        "        if len(indices[0]) > 0 and indices[0][0] != -1:\n",
        "             for i, idx in enumerate(indices[0]):\n",
        "                 if idx < len(cv_data) and idx not in relevant_indices_found: # Checagem de segurança e duplicatas\n",
        "                      doc = cv_data[idx]\n",
        "                      similarity = similarities[0][i] # Similaridade Cosseno\n",
        "\n",
        "                      # **** DEBUG: Imprimir o documento recuperado ANTES de filtrar ****\n",
        "                      print(f\"\\n  [DEBUG] Recuperado Doc Índice: {idx}, Similaridade: {similarity:.4f}\")\n",
        "                      print(f\"  [DEBUG] Categoria: {doc.get('Category', 'N/A')}, Sub: {doc.get('Subcategory', 'N/A')}\")\n",
        "                      print(f\"  [DEBUG] Descrição: {doc.get('Description', '')[:300]}...\") # Primeiros 300 chars\n",
        "                      # ****************************************************************\n",
        "\n",
        "                      # Filtrar por similaridade\n",
        "                      if similarity >= similarity_threshold:\n",
        "                          retrieved_docs_info.append({\n",
        "                              \"id\": doc.get(\"ID\", f\"index_{idx}\"),\n",
        "                              \"category\": doc.get(\"Category\", \"N/A\"),\n",
        "                              \"subcategory\": doc.get(\"Subcategory\", \"N/A\"),\n",
        "                              \"description\": doc.get(\"Description\", \"\"),\n",
        "                              \"similarity\": similarity # Armazenar similaridade em vez de distância\n",
        "                          })\n",
        "                          relevant_indices_found.append(idx)\n",
        "                          print(f\"    -> Documento {idx} INCLUÍDO (Similaridade {similarity:.4f} >= {similarity_threshold})\")\n",
        "                      else:\n",
        "                          print(f\"    -> Documento {idx} DESCARTADO (Similaridade {similarity:.4f} < {similarity_threshold})\")\n",
        "\n",
        "                 elif idx in relevant_indices_found:\n",
        "                      print(f\"  [DEBUG] Índice {idx} já processado, pulando.\")\n",
        "                 else:\n",
        "                      print(f\"  [DEBUG] AVISO: Índice FAISS {idx} inválido retornado.\", file=sys.stderr)\n",
        "\n",
        "        if not retrieved_docs_info:\n",
        "            print(\"[RAG] Nenhum documento relevante encontrado (ou todos abaixo do threshold) no CV para esta consulta.\")\n",
        "            # Tenta dar uma resposta mais útil se NADA foi recuperado\n",
        "            if len(indices[0]) > 0 and indices[0][0] != -1:\n",
        "                 # Pega o mais similar mesmo que abaixo do threshold para dar um feedback\n",
        "                 top_idx = indices[0][0]\n",
        "                 top_sim = similarities[0][0]\n",
        "                 top_doc_desc = cv_data[top_idx].get('Description', '')[:200]\n",
        "                 return (f\"Não encontrei informações altamente relevantes (acima do limiar de {similarity_threshold*100:.0f}% de similaridade) para isso no CV. \"\n",
        "                         f\"A seção mais próxima encontrada (similaridade {top_sim:.2f}) foi sobre: '{top_doc_desc}...', mas pode não ser o que você procura.\"), []\n",
        "            else:\n",
        "                 return \"Não encontrei nenhuma seção no CV que pareça relacionada a sua pergunta.\", []\n",
        "\n",
        "        print(f\"[RAG] Documentos relevantes (acima do threshold) selecionados: {len(retrieved_docs_info)}\")\n",
        "\n",
        "        # --- 4. Construir Contexto para o LLM ---\n",
        "        context_parts = []\n",
        "        for i, doc_info in enumerate(retrieved_docs_info):\n",
        "            # Deixar mais claro para o LLM de onde vem a informação\n",
        "            context_parts.append(f\"--- [INÍCIO Seção Relevante {i+1} do CV - ID: {doc_info['id']}, Similaridade: {doc_info['similarity']:.2f}] ---\\n\"\n",
        "                                 f\"{doc_info['description']}\\n\"\n",
        "                                 f\"--- [FIM Seção Relevante {i+1}] ---\")\n",
        "        context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "        # **** DEBUG: Imprimir o contexto final que vai para o LLM ****\n",
        "        print(\"\\n\" + \"=\"*20 + \" CONTEÚDO ENVIADO PARA O GEMINI \" + \"=\"*20)\n",
        "        print(f\"[DEBUG] Contexto Construído ({len(context)} caracteres):\\n{context}\")\n",
        "        print(\"=\"*60)\n",
        "        # ***********************************************************\n",
        "\n",
        "        # --- 5. Preparar Mensagens para Gemini (Prompt Ajustado) ---\n",
        "        system_prompt = (\n",
        "            \"Você é um assistente de IA para análise de currículos (CVs). \"\n",
        "            \"Sua tarefa é responder à pergunta do usuário baseando-se *prioritariamente* nas informações das seções do CV fornecidas entre [INÍCIO Seção Relevante] e [FIM Seção Relevante]. \"\n",
        "            \"Sintetize a informação encontrada nessas seções para fornecer uma resposta coesa. \"\n",
        "            \"Se a resposta estiver claramente presente, responda diretamente. \"\n",
        "            \"Se a informação não estiver nas seções fornecidas, ou se as seções não forem relevantes para a pergunta, informe que a resposta não foi encontrada *nesse contexto específico do CV*. \"\n",
        "            #\"Não adicione conhecimento externo.\"\n",
        "            \"Caso precise, utilize informação externa para trazer uma resposta mais completa, mas sem .\"\n",
        "            \"A sessão de description é o foco das informações.\"\n",
        "            # \"Mencione brevemente as seções do CV (por ID ou categoria) que você usou para basear sua resposta, se possível.\" # Instrução opcional\n",
        "        )\n",
        "        human_prompt = (\n",
        "            f\"**Contexto Extraído do Currículo:**\\n\"\n",
        "            f\"{context}\\n\\n\"\n",
        "            f\"**Pergunta do Usuário:**\\n{query}\\n\\n\"\n",
        "            f\"**Instrução Final:** Use o contexto acima para responder à pergunta.\"\n",
        "        )\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=system_prompt),\n",
        "            HumanMessage(content=human_prompt)\n",
        "        ]\n",
        "\n",
        "        # --- 6. Invocar o Modelo Gemini ---\n",
        "        print(\"[RAG] Enviando consulta para o modelo Gemini...\")\n",
        "        start_time = time.time()\n",
        "        response = chat_model.invoke(messages)\n",
        "        generation_time = time.time() - start_time\n",
        "        print(f\"[RAG] Resposta recebida do Gemini em {generation_time:.2f} seg.\")\n",
        "\n",
        "        # --- 7. Retornar Resultado ---\n",
        "        return response.content, retrieved_docs_info\n",
        "\n",
        "    except Exception as e:\n",
        "        # (Manter o tratamento de erro como estava)\n",
        "        print(f\"ERRO GERAL durante a execução da consulta RAG: {e}\", file=sys.stderr)\n",
        "        retrieved = retrieved_docs_info if 'retrieved_docs_info' in locals() else []\n",
        "        return f\"Ocorreu um erro inesperado ao processar sua pergunta: {e}\", retrieved"
      ],
      "metadata": {
        "id": "0sT-kwz8Nnwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 6: INTERAÇÃO COM O USUÁRIO\n",
        "\n",
        "\n",
        "# Verifica se tudo está pronto para começar\n",
        "if chat_model and embedder_model and faiss_index and cv_data:\n",
        "    print(\"\\n===================================\")\n",
        "    print(\" Assistente de Análise de CV pronto! \")\n",
        "    print(\"===================================\")\n",
        "    print(\"Digite sua pergunta sobre o currículo carregado ou 'sair' para terminar.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"\\nSua pergunta: \")\n",
        "        if user_query.lower().strip() == 'sair':\n",
        "            print(\"Encerrando o assistente. Até logo!\")\n",
        "            break\n",
        "        if not user_query.strip():\n",
        "            continue\n",
        "\n",
        "        # Chama a função RAG principal\n",
        "        answer, sources = ask_cv_assistant(user_query, k=4) # Pega os 4 mais relevantes\n",
        "\n",
        "        print(\"\\n--------------------\")\n",
        "        print(\"Resposta do Assistente:\")\n",
        "        print(answer)\n",
        "        print(\"--------------------\")\n",
        "\n",
        "        if sources:\n",
        "             print(\"\\nFontes utilizadas (Seções do CV):\")\n",
        "             for src in sources:\n",
        "                  print(f\"  - ID: {src['id']} (Distância: {src['distance']:.4f}) - {src['description'][:100]}...\") # Mostra início da descrição\n",
        "        print(\"--------------------\\n\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nERRO CRÍTICO: O assistente não pode ser iniciado.\", file=sys.stderr)\n",
        "    if not chat_model: print(\"- Modelo Gemini não inicializado.\", file=sys.stderr)\n",
        "    if not embedder_model: print(\"- Modelo de embedding não inicializado.\", file=sys.stderr)\n",
        "    if not faiss_index: print(\"- Índice FAISS não inicializado/carregado.\", file=sys.stderr)\n",
        "    if not cv_data: print(\"- Dados do CV não carregados.\", file=sys.stderr)\n",
        "    print(\"Verifique as mensagens de erro nas células anteriores, especialmente a configuração da API Key, caminhos de arquivos e montagem do Drive.\", file=sys.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "2rXuQw_fSww3",
        "outputId": "386c3284-e9fe-47b7-bc2b-166ea37e6e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================\n",
            " Assistente de Análise de CV pronto! \n",
            "===================================\n",
            "Digite sua pergunta sobre o currículo carregado ou 'sair' para terminar.\n",
            "\n",
            "Sua pergunta: Me traga informações desse curriculo realcionadas a React\n",
            "\n",
            "[RAG] Processando consulta: 'Me traga informações desse curriculo realcionadas a React'\n",
            "[RAG] Consulta codificada e normalizada em 0.025 seg.\n",
            "[RAG] Busca FAISS concluída em 0.000 seg. Encontrados 4 vizinhos.\n",
            "\n",
            "  [DEBUG] Recuperado Doc Índice: 2, Similaridade: 0.4934\n",
            "  [DEBUG] Categoria: Skills, Sub: Technical\n",
            "  [DEBUG] Descrição: React.js...\n",
            "    -> Documento 2 INCLUÍDO (Similaridade 0.4934 >= 0.45)\n",
            "\n",
            "  [DEBUG] Recuperado Doc Índice: 16, Similaridade: 0.3142\n",
            "  [DEBUG] Categoria: Experience, Sub: Engenheiro Mecânico\n",
            "  [DEBUG] Descrição: Callidus Engenharia, Mechanical Engineer (March 2021 - May 2022). Conducted safety inspections (NR-13, NR-12, NR-10), designed HVAC systems (climate control, refrigeration, air renewal), prepared technical reports, and worked on industrial projects (electrical, hydraulic, mechanical, piping)....\n",
            "    -> Documento 16 DESCARTADO (Similaridade 0.3142 < 0.45)\n",
            "\n",
            "  [DEBUG] Recuperado Doc Índice: 11, Similaridade: 0.2863\n",
            "  [DEBUG] Categoria: Certifications, Sub: Certification\n",
            "  [DEBUG] Descrição: Inicio do projeto: Como começar um projeto bem-sucedido...\n",
            "    -> Documento 11 DESCARTADO (Similaridade 0.2863 < 0.45)\n",
            "\n",
            "  [DEBUG] Recuperado Doc Índice: 9, Similaridade: 0.2674\n",
            "  [DEBUG] Categoria: Certifications, Sub: Certification\n",
            "  [DEBUG] Descrição: Certificado de Participação - IA Generativa na Pesquisa Acadêmica: Introdução e Prática da Engenharia de Prompt...\n",
            "    -> Documento 9 DESCARTADO (Similaridade 0.2674 < 0.45)\n",
            "[RAG] Documentos relevantes (acima do threshold) selecionados: 1\n",
            "\n",
            "==================== CONTEÚDO ENVIADO PARA O GEMINI ====================\n",
            "[DEBUG] Contexto Construído (109 caracteres):\n",
            "--- [INÍCIO Seção Relevante 1 do CV - ID: 3, Similaridade: 0.49] ---\n",
            "React.js\n",
            "--- [FIM Seção Relevante 1] ---\n",
            "============================================================\n",
            "[RAG] Enviando consulta para o modelo Gemini...\n",
            "[RAG] Resposta recebida do Gemini em 0.91 seg.\n",
            "\n",
            "--------------------\n",
            "Resposta do Assistente:\n",
            "O currículo menciona experiência com React.js.\n",
            "--------------------\n",
            "\n",
            "Fontes utilizadas (Seções do CV):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'distance'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b44bdaa92367>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m              \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFontes utilizadas (Seções do CV):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m              \u001b[0;32mfor\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  - ID: {src['id']} (Distância: {src['distance']:.4f}) - {src['description'][:100]}...\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Mostra início da descrição\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'distance'"
          ]
        }
      ]
    }
  ]
}
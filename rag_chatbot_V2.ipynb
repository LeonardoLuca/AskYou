{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPZpUX2vwRP1jnqXsJ2ycM2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeonardoLuca/AskYou/blob/main/rag_chatbot_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook Colab com RAG e Gemini Integrados"
      ],
      "metadata": {
        "id": "Hh8uXjXxbQFF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KZFMCJ_yKkTZ"
      },
      "outputs": [],
      "source": [
        "# CÉLULA 1: INSTALAÇÕES\n",
        "\n",
        "# Instala todas as bibliotecas necessárias de uma vez\n",
        "!pip install -q langchain langchain-google-genai sentence-transformers faiss-cpu torch google-colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B-PfPlKnggFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 2: IMPORTS E CONFIGURAÇÃO DA API KEY\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import faiss\n",
        "from google.colab import userdata, drive\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# --- Configuração da Chave de API do Google ---\n",
        "GOOGLE_API_KEY = None\n",
        "try:\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    if not GOOGLE_API_KEY:\n",
        "        print(\"AVISO: Chave 'GOOGLE_API_KEY' encontrada nos Secrets, mas está vazia.\", file=sys.stderr)\n",
        "        GOOGLE_API_KEY = None # Garante que é None se estiver vazia\n",
        "    else:\n",
        "        print(\"Chave de API do Google carregada com sucesso!\")\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"ERRO: Secret 'GOOGLE_API_KEY' não encontrado.\", file=sys.stderr)\n",
        "    print(\"Por favor, adicione sua chave de API do Google AI Studio aos Secrets do Colab.\", file=sys.stderr)\n",
        "except Exception as e:\n",
        "    print(f\"ERRO ao buscar a chave de API: {e}\", file=sys.stderr)\n",
        "\n",
        "# Limpa a variável de ambiente se existir (boa prática)\n",
        "if 'GOOGLE_API_KEY' in os.environ:\n",
        "    del os.environ['GOOGLE_API_KEY']"
      ],
      "metadata": {
        "id": "06Lg-QRmL67q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1276bbc-b2bc-4fa0-dd24-dd37044374db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chave de API do Google carregada com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 3: INICIALIZAÇÃO DOS MODELOS (GEMINI E EMBEDDING)\n",
        "\n",
        "# --- Inicializa o Modelo Gemini ---\n",
        "chat_model = None\n",
        "if GOOGLE_API_KEY:\n",
        "    try:\n",
        "        # Use o modelo Gemini desejado (Pro ou Flash)\n",
        "        chat_model = ChatGoogleGenerativeAI(\n",
        "            model=\"gemini-2.0-flash-001\", # Ou \"gemini-1.5-flash-latest\" para mais rápido/barato\n",
        "            google_api_key=GOOGLE_API_KEY,\n",
        "            temperature=0.4, # Reduz a criatividade para respostas mais factuais Variabilidade\n",
        "            # safety_settings=... # Adicione configurações de segurança se necessário\n",
        "        )\n",
        "        print(f\"Modelo Gemini '{chat_model.model}' instanciado com sucesso.\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERRO ao instanciar o modelo ChatGoogleGenerativeAI: {e}\", file=sys.stderr)\n",
        "else:\n",
        "    print(\"AVISO: Modelo Gemini não será instanciado pois a chave de API não foi carregada.\", file=sys.stderr)\n",
        "\n",
        "# --- Inicializa o Modelo de Embedding ---\n",
        "embedder_model = None\n",
        "try:\n",
        "    print(\"Carregando modelo de embedding (SentenceTransformer)...\")\n",
        "    start_time = time.time()\n",
        "    # Modelo leve e eficaz para semântica geral\n",
        "    embedder_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    print(f\"Modelo de embedding carregado em {time.time() - start_time:.2f} segundos.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERRO ao carregar o modelo de embedding: {e}\", file=sys.stderr)"
      ],
      "metadata": {
        "id": "w_sy9Z4FNLwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f2dc682-162d-4a63-ef48-0952bf566c72"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo Gemini 'models/gemini-2.0-flash-001' instanciado com sucesso.\n",
            "Carregando modelo de embedding (SentenceTransformer)...\n",
            "Modelo de embedding carregado em 1.21 segundos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================\n",
        "#             CÉLULA 4: MONTAGEM DO DRIVE E CARREGAMENTO/INDEXAÇÃO DO CV (COM IndexFlatIP)\n",
        "# ===================================================\n",
        "\n",
        "# --- Monta o Google Drive ---\n",
        "# (Como estava antes)\n",
        "try:\n",
        "    drive.mount(\"/content/drive\", force_remount=True)\n",
        "    print(\"Google Drive montado com sucesso em /content/drive.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERRO ao montar o Google Drive: {e}\", file=sys.stderr)\n",
        "\n",
        "# --- Define Caminhos (AJUSTE SE NECESSÁRIO) ---\n",
        "# (Como estava antes)\n",
        "DRIVE_BASE_PATH = \"/content/drive/My Drive/Colab Notebooks\" # Exemplo, ajuste!\n",
        "CV_JSON_PATH = os.path.join(DRIVE_BASE_PATH, \"datasets/dataset-cv-leonardo.json\")\n",
        "# **Mudar o nome do arquivo de índice para refletir a mudança (opcional, mas recomendado)**\n",
        "FAISS_INDEX_PATH = os.path.join(DRIVE_BASE_PATH, \"index/faiss-cv-leonardo-index-IP.bin\") # <-- MUDOU O NOME\n",
        "\n",
        "# --- Variáveis para os dados e índice ---\n",
        "# (Como estava antes)\n",
        "cv_data = None\n",
        "cv_descriptions = None\n",
        "cv_categories = None\n",
        "faiss_index = None\n",
        "\n",
        "# --- Carrega o Dataset (CV JSON) ---\n",
        "# (Como estava antes - carregar cv_data e cv_descriptions)\n",
        "if os.path.exists(CV_JSON_PATH):\n",
        "    try:\n",
        "        with open(CV_JSON_PATH, 'r', encoding='utf-8') as f:\n",
        "            cv_data = json.load(f)\n",
        "        print(f\"Dataset CV carregado de: {CV_JSON_PATH}\")\n",
        "        cv_descriptions = [entry.get('Description', '') for entry in cv_data]\n",
        "        cv_categories = [entry.get('Category', '') for entry in cv_data]\n",
        "        print(f\"Extraídas {len(cv_descriptions)} descrições do CV. \\n Extraídas {len(cv_categories)} categorias do CV.\")\n",
        "        if not cv_descriptions:\n",
        "             print(\"AVISO: Nenhuma descrição encontrada no arquivo JSON.\", file=sys.stderr)\n",
        "             cv_data = None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"ERRO: O arquivo {CV_JSON_PATH} não é um JSON válido.\", file=sys.stderr)\n",
        "    except Exception as e:\n",
        "        print(f\"ERRO ao carregar ou processar o arquivo JSON do CV: {e}\", file=sys.stderr)\n",
        "else:\n",
        "    print(f\"ERRO: Arquivo do dataset CV não encontrado em: {CV_JSON_PATH}. Verifique o caminho.\", file=sys.stderr)\n",
        "\n",
        "\n",
        "# --- Cria/Carrega Embeddings e Índice FAISS (usando IndexFlatIP) ---\n",
        "if cv_data and embedder_model:\n",
        "    try:\n",
        "        # Verifica se o índice FAISS (versão IP) já existe\n",
        "        if os.path.exists(FAISS_INDEX_PATH):\n",
        "            print(f\"Tentando carregar índice FAISS existente (IP) de: {FAISS_INDEX_PATH}\")\n",
        "            faiss_index = faiss.read_index(FAISS_INDEX_PATH)\n",
        "            # Validações (como antes)\n",
        "            if faiss_index.d != embedder_model.get_sentence_embedding_dimension():\n",
        "                 print(f\"AVISO: Dimensão do índice ({faiss_index.d}) diferente. Recriando.\", file=sys.stderr)\n",
        "                 faiss_index = None\n",
        "            elif faiss_index.ntotal != len(cv_descriptions) and len(cv_descriptions):\n",
        "                 print(f\"AVISO: Número de vetores ({faiss_index.ntotal}) diferente. Recriando.\", file=sys.stderr)\n",
        "                 faiss_index = None\n",
        "            else:\n",
        "                 print(f\"Índice FAISS (IP) carregado com sucesso ({faiss_index.ntotal} vetores).\")\n",
        "\n",
        "        # Se não carregou, cria um novo\n",
        "        if faiss_index is None:\n",
        "            print(\"Gerando embeddings para as descrições do CV...\")\n",
        "            start_time = time.time()\n",
        "            embeddings = embedder_model.encode(cv_descriptions, show_progress_bar=True)\n",
        "            embeddings = np.array(embeddings).astype('float32')\n",
        "\n",
        "            # **** NORMALIZAÇÃO DOS EMBEDDINGS ****\n",
        "            print(\"Normalizando embeddings (L2 norm)...\")\n",
        "            faiss.normalize_L2(embeddings)\n",
        "            # *************************************\n",
        "\n",
        "            dimension = embeddings.shape[1]\n",
        "            print(f\"Criando novo índice FAISS com IndexFlatIP (Similaridade Cosseno), dimensão {dimension}...\")\n",
        "            # **** USAR IndexFlatIP ****\n",
        "            faiss_index = faiss.IndexFlatIP(dimension)\n",
        "            # **************************\n",
        "            faiss_index.add(embeddings) # Adiciona os embeddings NORMALIZADOS\n",
        "            print(f\"Índice FAISS (IP) criado e populado com {faiss_index.ntotal} vetores.\")\n",
        "\n",
        "            # Salva o novo índice (IP)\n",
        "            try:\n",
        "                print(f\"Salvando índice FAISS (IP) em: {FAISS_INDEX_PATH}\")\n",
        "                os.makedirs(os.path.dirname(FAISS_INDEX_PATH), exist_ok=True)\n",
        "                faiss.write_index(faiss_index, FAISS_INDEX_PATH)\n",
        "                print(\"Índice FAISS (IP) salvo com sucesso.\")\n",
        "            except Exception as e:\n",
        "                print(f\"ERRO ao salvar o índice FAISS (IP): {e}\", file=sys.stderr)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERRO durante a geração de embeddings ou manipulação do índice FAISS (IP): {e}\", file=sys.stderr)\n",
        "        faiss_index = None\n",
        "\n",
        "elif not embedder_model:\n",
        "     print(\"AVISO: Processo de indexação pulado (modelo de embedding não carregado).\", file=sys.stderr)\n",
        "else:\n",
        "     print(\"AVISO: Processo de indexação pulado (dados do CV não carregados).\", file=sys.stderr)"
      ],
      "metadata": {
        "id": "tQ6-pvMpMS-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d22c8f6a-946f-4e2c-de9e-443f90ffa8b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive montado com sucesso em /content/drive.\n",
            "Dataset CV carregado de: /content/drive/My Drive/Colab Notebooks/datasets/dataset-cv-leonardo.json\n",
            "Extraídas 27 descrições do CV. \n",
            " Extraídas 27 descrições do CV.\n",
            "Tentando carregar índice FAISS existente (IP) de: /content/drive/My Drive/Colab Notebooks/index/faiss-cv-leonardo-index-IP.bin\n",
            "Índice FAISS (IP) carregado com sucesso (27 vetores).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================\n",
        "#             CÉLULA 5: FUNÇÃO RAG PRINCIPAL (COM MAIS DEBUG)\n",
        "# ===================================================\n",
        "\n",
        "def ask_cv_assistant(query, k=3, similarity_threshold=0.45): # <-- Ajustado threshold para Cosine\n",
        "    \"\"\"\n",
        "    Realiza uma consulta RAG no CV carregado.\n",
        "    USA SIMILARIDADE COSSENO (requer IndexFlatIP e normalização - veja Célula 4 modificada)\n",
        "\n",
        "    Args:\n",
        "        query (str): A pergunta do usuário sobre o CV.\n",
        "        k (int): Número máximo de seções relevantes a recuperar.\n",
        "        similarity_threshold (float): Limiar de similaridade cosseno (0 a 1).\n",
        "                                      Valores mais altos indicam maior similaridade.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (str: resposta_gerada, list: documentos_recuperados)\n",
        "               Retorna mensagens de erro na string de resposta em caso de falha.\n",
        "    \"\"\"\n",
        "    # --- Verificações Iniciais ---\n",
        "    # (Mantenha as verificações como estavam)\n",
        "    if not chat_model: return \"ERRO: O modelo Gemini não está pronto.\", []\n",
        "    if not embedder_model: return \"ERRO: O modelo de embedding não está pronto.\", []\n",
        "    if not faiss_index: return \"ERRO: O índice FAISS (CV) não está pronto.\", []\n",
        "    if not cv_data: return \"ERRO: Os dados do CV não foram carregados.\", []\n",
        "\n",
        "    print(f\"\\n[RAG] Processando consulta: '{query}'\")\n",
        "\n",
        "    try:\n",
        "        # --- 1. Codificar a Consulta e NORMALIZAR ---\n",
        "        start_time = time.time()\n",
        "        query_embedding = embedder_model.encode([query])\n",
        "        query_embedding = np.array(query_embedding).astype('float32')\n",
        "        faiss.normalize_L2(query_embedding) # Normaliza para usar com IndexFlatIP (Cosine Similarity)\n",
        "        print(f\"[RAG] Consulta codificada e normalizada em {time.time() - start_time:.3f} seg.\")\n",
        "\n",
        "        # --- 2. Buscar no FAISS (usando IndexFlatIP) ---\n",
        "        start_time = time.time()\n",
        "        # search retorna similaridades (Inner Product/Cosine) e índices\n",
        "        similarities, indices = faiss_index.search(query_embedding, k)\n",
        "        print(f\"[RAG] Busca FAISS concluída em {time.time() - start_time:.3f} seg. Encontrados {len(indices[0])} vizinhos.\")\n",
        "\n",
        "        # --- 3. Recuperar e Filtrar Documentos (usando similaridade) ---\n",
        "        retrieved_docs_info = []\n",
        "        relevant_indices_found = [] # Para evitar duplicatas se k for grande\n",
        "        if len(indices[0]) > 0 and indices[0][0] != -1:\n",
        "             for i, idx in enumerate(indices[0]):\n",
        "                 if idx < len(cv_data) and idx not in relevant_indices_found: # Checagem de segurança e duplicatas\n",
        "                      doc = cv_data[idx]\n",
        "                      similarity = similarities[0][i] # Similaridade Cosseno\n",
        "\n",
        "                      # **** DEBUG: Imprimir o documento recuperado ANTES de filtrar ****\n",
        "                      print(f\"\\n  [DEBUG] Recuperado Doc Índice: {idx}, Similaridade: {similarity:.4f}\")\n",
        "                      print(f\"  [DEBUG] Categoria: {doc.get('Category', 'N/A')}, Sub: {doc.get('Subcategory', 'N/A')}\")\n",
        "                      print(f\"  [DEBUG] Descrição: {doc.get('Description', '')[:300]}...\") # Primeiros 300 chars\n",
        "                      # ****************************************************************\n",
        "\n",
        "                      # Filtrar por similaridade\n",
        "                      if similarity >= similarity_threshold:\n",
        "                          retrieved_docs_info.append({\n",
        "                              \"id\": doc.get(\"ID\", f\"index_{idx}\"),\n",
        "                              \"category\": doc.get(\"Category\", \"N/A\"),\n",
        "                              \"subcategory\": doc.get(\"Subcategory\", \"N/A\"),\n",
        "                              \"description\": doc.get(\"Description\", \"\"),\n",
        "                              \"similarity\": similarity # Armazenar similaridade em vez de distância\n",
        "                          })\n",
        "                          relevant_indices_found.append(idx)\n",
        "                          print(f\"    -> Documento {idx} INCLUÍDO (Similaridade {similarity:.4f} >= {similarity_threshold})\")\n",
        "                      else:\n",
        "                          print(f\"    -> Documento {idx} DESCARTADO (Similaridade {similarity:.4f} < {similarity_threshold})\")\n",
        "\n",
        "                 elif idx in relevant_indices_found:\n",
        "                      print(f\"  [DEBUG] Índice {idx} já processado, pulando.\")\n",
        "                 else:\n",
        "                      print(f\"  [DEBUG] AVISO: Índice FAISS {idx} inválido retornado.\", file=sys.stderr)\n",
        "\n",
        "        if not retrieved_docs_info:\n",
        "            print(\"[RAG] Nenhum documento relevante encontrado (ou todos abaixo do threshold) no CV para esta consulta.\")\n",
        "            # Tenta dar uma resposta mais útil se NADA foi recuperado\n",
        "            if len(indices[0]) > 0 and indices[0][0] != -1:\n",
        "                 # Pega o mais similar mesmo que abaixo do threshold para dar um feedback\n",
        "                 top_idx = indices[0][0]\n",
        "                 top_sim = similarities[0][0]\n",
        "                 top_doc_desc = cv_data[top_idx].get('Description', '')[:200]\n",
        "                 return (f\"Não encontrei informações altamente relevantes (acima do limiar de {similarity_threshold*100:.0f}% de similaridade) para isso no CV. \"\n",
        "                         f\"A seção mais próxima encontrada (similaridade {top_sim:.2f}) foi sobre: '{top_doc_desc}...', mas pode não ser o que você procura.\"), []\n",
        "            else:\n",
        "                 return \"Não encontrei nenhuma seção no CV que pareça relacionada a sua pergunta.\", []\n",
        "\n",
        "        print(f\"[RAG] Documentos relevantes (acima do threshold) selecionados: {len(retrieved_docs_info)}\")\n",
        "\n",
        "        # --- 4. Construir Contexto para o LLM ---\n",
        "        context_parts = []\n",
        "        for i, doc_info in enumerate(retrieved_docs_info):\n",
        "            # Deixar mais claro para o LLM de onde vem a informação\n",
        "            context_parts.append(f\"--- [INÍCIO Seção Relevante {i+1} do CV - ID: {doc_info['id']}, Similaridade: {doc_info['similarity']:.2f}] ---\\n\"\n",
        "                                 f\"{doc_info['description']}\\n\"\n",
        "                                 f\"--- [FIM Seção Relevante {i+1}] ---\")\n",
        "        context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "        # **** DEBUG: Imprimir o contexto final que vai para o LLM ****\n",
        "        print(\"\\n\" + \"=\"*20 + \" CONTEÚDO ENVIADO PARA O GEMINI \" + \"=\"*20)\n",
        "        print(f\"[DEBUG] Contexto Construído ({len(context)} caracteres):\\n{context}\")\n",
        "        print(\"=\"*60)\n",
        "        # ***********************************************************\n",
        "\n",
        "        # --- 5. Preparar Mensagens para Gemini (Prompt Ajustado) ---\n",
        "        system_prompt = (\n",
        "            \"Você é um assistente de IA para análise de currículos (CVs). \"\n",
        "            \"Sua tarefa é responder à pergunta do usuário baseando-se *prioritariamente* nas informações das seções do CV fornecidas entre [INÍCIO Seção Relevante] e [FIM Seção Relevante]. \"\n",
        "            \"Sintetize a informação encontrada nessas seções para fornecer uma resposta coesa. \"\n",
        "            \"Se a resposta estiver claramente presente, responda diretamente. \"\n",
        "            \"Se a informação não estiver nas seções fornecidas, ou se as seções não forem relevantes para a pergunta, informe que a resposta não foi encontrada *nesse contexto específico do CV*. \"\n",
        "            #\"Não adicione conhecimento externo.\"\n",
        "            \"Caso precise, utilize informação externa para trazer uma resposta mais completa, mas sem .\"\n",
        "            \"A sessão de description é o foco das informações.\"\n",
        "            # \"Mencione brevemente as seções do CV (por ID ou categoria) que você usou para basear sua resposta, se possível.\" # Instrução opcional\n",
        "        )\n",
        "        human_prompt = (\n",
        "            f\"**Contexto Extraído do Currículo:**\\n\"\n",
        "            f\"{context}\\n\\n\"\n",
        "            f\"**Pergunta do Usuário:**\\n{query}\\n\\n\"\n",
        "            f\"**Instrução Final:** Use o contexto acima para responder à pergunta.\"\n",
        "        )\n",
        "\n",
        "        messages = [\n",
        "            SystemMessage(content=system_prompt),\n",
        "            HumanMessage(content=human_prompt)\n",
        "        ]\n",
        "\n",
        "        # --- 6. Invocar o Modelo Gemini ---\n",
        "        print(\"[RAG] Enviando consulta para o modelo Gemini...\")\n",
        "        start_time = time.time()\n",
        "        response = chat_model.invoke(messages)\n",
        "        generation_time = time.time() - start_time\n",
        "        print(f\"[RAG] Resposta recebida do Gemini em {generation_time:.2f} seg.\")\n",
        "\n",
        "        # --- 7. Retornar Resultado ---\n",
        "        return response.content, retrieved_docs_info\n",
        "\n",
        "    except Exception as e:\n",
        "        # (Manter o tratamento de erro como estava)\n",
        "        print(f\"ERRO GERAL durante a execução da consulta RAG: {e}\", file=sys.stderr)\n",
        "        retrieved = retrieved_docs_info if 'retrieved_docs_info' in locals() else []\n",
        "        return f\"Ocorreu um erro inesperado ao processar sua pergunta: {e}\", retrieved"
      ],
      "metadata": {
        "id": "0sT-kwz8Nnwv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CÉLULA 6: INTERAÇÃO COM O USUÁRIO\n",
        "\n",
        "\n",
        "# Verifica se tudo está pronto para começar\n",
        "if chat_model and embedder_model and faiss_index and cv_data:\n",
        "    print(\"\\n===================================\")\n",
        "    print(\" Assistente de Análise de CV pronto! \")\n",
        "    print(\"===================================\")\n",
        "    print(\"Digite sua pergunta sobre o currículo carregado ou 'sair' para terminar.\")\n",
        "\n",
        "    while True:\n",
        "        user_query = input(\"\\nSua pergunta: \")\n",
        "        if user_query.lower().strip() == 'sair':\n",
        "            print(\"Encerrando o assistente. Até logo!\")\n",
        "            break\n",
        "        if not user_query.strip():\n",
        "            continue\n",
        "\n",
        "        # Chama a função RAG principal\n",
        "        answer, sources = ask_cv_assistant(user_query, k=4) # Pega os 4 mais relevantes\n",
        "\n",
        "        print(\"\\n--------------------\")\n",
        "        print(\"Resposta do Assistente:\")\n",
        "        print(answer)\n",
        "        print(\"--------------------\")\n",
        "\n",
        "        if sources:\n",
        "             print(\"\\nFontes utilizadas (Seções do CV):\")\n",
        "             for src in sources:\n",
        "                  print(f\"  - ID: {src['id']} (Distância: {src['distance']:.4f}) - {src['description'][:100]}...\") # Mostra início da descrição\n",
        "        print(\"--------------------\\n\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nERRO CRÍTICO: O assistente não pode ser iniciado.\", file=sys.stderr)\n",
        "    if not chat_model: print(\"- Modelo Gemini não inicializado.\", file=sys.stderr)\n",
        "    if not embedder_model: print(\"- Modelo de embedding não inicializado.\", file=sys.stderr)\n",
        "    if not faiss_index: print(\"- Índice FAISS não inicializado/carregado.\", file=sys.stderr)\n",
        "    if not cv_data: print(\"- Dados do CV não carregados.\", file=sys.stderr)\n",
        "    print(\"Verifique as mensagens de erro nas células anteriores, especialmente a configuração da API Key, caminhos de arquivos e montagem do Drive.\", file=sys.stderr)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2rXuQw_fSww3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47dfa6bf-ddf6-4442-c424-d83581c5ed76"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================\n",
            " Assistente de Análise de CV pronto! \n",
            "===================================\n",
            "Digite sua pergunta sobre o currículo carregado ou 'sair' para terminar.\n",
            "\n",
            "Sua pergunta: algo sobre react?\n",
            "\n",
            "[RAG] Processando consulta: 'algo sobre react?'\n",
            "[RAG] Consulta codificada e normalizada em 0.026 seg.\n",
            "[RAG] Busca FAISS concluída em 0.000 seg. Encontrados 4 vizinhos.\n",
            "\n",
            "  [DEBUG] Recuperado Doc Índice: 11, Similaridade: 0.3139\n",
            "  [DEBUG] Categoria: Certifications, Sub: Certification\n",
            "  [DEBUG] Descrição: Inicio do projeto: Como começar um projeto bem-sucedido...\n",
            "    -> Documento 11 DESCARTADO (Similaridade 0.3139 < 0.45)\n",
            "\n",
            "  [DEBUG] Recuperado Doc Índice: 5, Similaridade: 0.3105\n",
            "  [DEBUG] Categoria: Languages, Sub: Language Proficiency\n",
            "  [DEBUG] Descrição: Spanish (Limited Working)...\n",
            "    -> Documento 5 DESCARTADO (Similaridade 0.3105 < 0.45)\n",
            "\n",
            "  [DEBUG] Recuperado Doc Índice: 8, Similaridade: 0.2916\n",
            "  [DEBUG] Categoria: Certifications, Sub: Certification\n",
            "  [DEBUG] Descrição: Treinamento LGPD...\n",
            "    -> Documento 8 DESCARTADO (Similaridade 0.2916 < 0.45)\n",
            "\n",
            "  [DEBUG] Recuperado Doc Índice: 2, Similaridade: 0.2563\n",
            "  [DEBUG] Categoria: Skills, Sub: Technical\n",
            "  [DEBUG] Descrição: React.js...\n",
            "    -> Documento 2 DESCARTADO (Similaridade 0.2563 < 0.45)\n",
            "[RAG] Nenhum documento relevante encontrado (ou todos abaixo do threshold) no CV para esta consulta.\n",
            "\n",
            "--------------------\n",
            "Resposta do Assistente:\n",
            "Não encontrei informações altamente relevantes (acima do limiar de 45% de similaridade) para isso no CV. A seção mais próxima encontrada (similaridade 0.31) foi sobre: 'Inicio do projeto: Como começar um projeto bem-sucedido...', mas pode não ser o que você procura.\n",
            "--------------------\n",
            "--------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b44bdaa92367>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0muser_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSua pergunta: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sair'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Encerrando o assistente. Até logo!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}